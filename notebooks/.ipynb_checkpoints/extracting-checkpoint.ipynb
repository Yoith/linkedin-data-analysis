{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9856289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoith\\AppData\\Local\\Temp\\ipykernel_13296\\109443032.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342e9d8",
   "metadata": {},
   "source": [
    "# LinkedIn Job Scraper: Extracts job details and links from LinkedIn job search pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec17327f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Data Analyst' in 'Spain', were found 105 results.\n",
      "For 'Data Analyst' in 'England', were found 105 results.\n",
      "For 'Data Analyst' in 'Scotland', were found 42 results.\n",
      "For 'Data Engineer' in 'Spain', were found 105 results.\n",
      "For 'Data Engineer' in 'England', were found 105 results.\n",
      "For 'Data Engineer' in 'Scotland', were found 52 results.\n",
      "For 'Data Scientist' in 'Spain', were found 105 results.\n",
      "For 'Data Scientist' in 'England', were found 105 results.\n",
      "For 'Data Scientist' in 'Scotland', were found 14 results.\n"
     ]
    }
   ],
   "source": [
    "def scrape_linkedin_jobs(num_pages=15):\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn job search pages for specific keywords and locations.\n",
    "    \n",
    "    :param num_pages: Number of job search pages to scrape (default 25).\n",
    "    :return: List of BeautifulSoup objects for each job page scraped, and a Data Frame with all the links of job pages\n",
    "    \"\"\"\n",
    "    # Keywords and Locations\n",
    "    keywords = [\n",
    "        \"Data Analyst\",\n",
    "        \"Data Engineer\",\n",
    "        \"Data Scientist\"\n",
    "    ]\n",
    "\n",
    "    locations = [\n",
    "        \"Spain\",\n",
    "        \"England\",\n",
    "        \"Scotland\",\n",
    "    ]\n",
    "    \n",
    "    # Get login credentials\n",
    "    email = os.getenv(\"email\")\n",
    "    password = os.getenv(\"password\")\n",
    "    \n",
    "    # WebDriver setup\n",
    "    options = webdriver.ChromeOptions() \n",
    "    options.add_argument('--incognito')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(random.randint(1, 3))  \n",
    "    \n",
    "    # Fill login credentials\n",
    "    email_box = driver.find_element(By.ID, \"username\")\n",
    "    password_box = driver.find_element(By.ID, \"password\")\n",
    "    email_box.send_keys(email)\n",
    "    password_box.send_keys(password)\n",
    "    password_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(random.randint(1, 7))  \n",
    "\n",
    "    # List to store BeautifulSoup objects of job pages\n",
    "    soup_list = []\n",
    "\n",
    "    # List to store dictionaries of job details\n",
    "    job_details = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        for location in locations:\n",
    "            # Build LinkedIn job search URL\n",
    "            base_url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start=\"\n",
    "\n",
    "            new_soup_list = []\n",
    "\n",
    "            for page in range(num_pages):\n",
    "                page_url = base_url + str(page * 25)\n",
    "                driver.get(page_url)\n",
    "                time.sleep(random.uniform(3, 7))\n",
    "\n",
    "                # Extract job links\n",
    "                links = driver.find_elements(By.CSS_SELECTOR, \"a.job-card-container__link.job-card-list__title\")\n",
    "                job_links = [link.get_attribute(\"href\") for link in links]\n",
    "\n",
    "                for job_link in job_links:\n",
    "                    driver.get(job_link)\n",
    "                    time.sleep(random.uniform(3, 7))\n",
    "                    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "                    new_soup_list.append(soup)  # Append current soup to the list\n",
    "\n",
    "                    # Append job details to list of dictionaries\n",
    "                    job_details.append({'Keyword': keyword, 'Location': location, 'Job_Link': job_link})\n",
    "\n",
    "            print(f\"For '{keyword}' in '{location}', were found {len(new_soup_list)} results.\")\n",
    "            soup_list.extend(new_soup_list)\n",
    "\n",
    "     # Save soup_list using pickle\n",
    "    with open('soup_list.pkl', 'wb') as f:\n",
    "        pickle.dump(soup_list, f)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame and save it in a .csv\n",
    "    df_links = pd.DataFrame(job_details)\n",
    "    df_links.to_csv(\"data/links_and_query.csv\", index=False)\n",
    "    \n",
    "    return soup_list, df_links\n",
    "\n",
    "# Execute the function\n",
    "soup_list, df_links = scrape_linkedin_jobs(num_pages=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe13642",
   "metadata": {},
   "source": [
    "# Load Soup List from Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "69db92e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open('soup_list.pkl', 'rb') as f:\\nsoup_list = pickle.load(f)\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This part of the code, load the file with the soup information in case of need it\n",
    "\n",
    "\"\"\"with open('soup_list.pkl', 'rb') as f:\n",
    "soup_list = pickle.load(f)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d43be",
   "metadata": {},
   "source": [
    "# LinkedIn Job Offer Extractor: Extracts job details from BeautifulSoup objects and saves to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b064d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: 'NoneType' object has no attribute 'getText'\n",
      "Error: list index out of range\n"
     ]
    }
   ],
   "source": [
    "def extract_job_info(soup_list):\n",
    "    \"\"\"\n",
    "    Extract job information from a list of Beautiful Soup objects and save it to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        soup_list (list): List of Beautiful Soup objects containing job offer information.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing job offer information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to store job offer dictionaries\n",
    "    offers = []\n",
    "\n",
    "    # Iterate through each job offer\n",
    "    for soup in soup_list:\n",
    "        # Dictionary to store information for each job offer\n",
    "        offer_dict = {\n",
    "            \"title\": np.nan,\n",
    "            \"company\": np.nan,\n",
    "            \"link\": np.nan,\n",
    "            \"c_\": np.nan,\n",
    "            \"l_\": np.nan,\n",
    "            \"posted_date\": np.nan,  # Changing column name from a_ to posted_date\n",
    "            \"applications\": np.nan,\n",
    "            \"job_modality\": np.nan,\n",
    "            \"f_p_time\": np.nan,\n",
    "            \"job_responsibility\": np.nan,\n",
    "            \"job_skills\": np.nan,\n",
    "            \"job_salary\": np.nan,\n",
    "            \"scraped_on\": date.today()\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Extracting company name and job title\n",
    "            company = soup.find('div', class_='p5')\n",
    "            title = soup.find('h1', class_='t-24 t-bold job-details-jobs-unified-top-card__job-title').getText()\n",
    "            sub = soup.find('div', class_='job-details-jobs-unified-top-card__primary-description-container')\n",
    "\n",
    "            # Extracting link to the job posting\n",
    "            if sub.find(\"a\"):\n",
    "                link = sub.find(\"a\").get(\"href\")\n",
    "                offer_dict[\"link\"] = link\n",
    "\n",
    "            # Extracting company name\n",
    "            if sub.find(\"a\"):\n",
    "                company_name = sub.find(\"a\").getText()\n",
    "                offer_dict[\"company\"] = company_name\n",
    "\n",
    "            # Extracting more job details\n",
    "            more_info = sub.getText().strip().split(\"·\")\n",
    "\n",
    "            c_ = more_info[0].strip()\n",
    "            l_ = more_info[1].strip()\n",
    "            posted_date = more_info[2].strip()  # Changing variable name from a_ to posted_date\n",
    "\n",
    "            offer_dict[\"title\"] = title\n",
    "            offer_dict[\"c_\"] = c_\n",
    "            offer_dict[\"l_\"] = l_\n",
    "            offer_dict[\"posted_date\"] = posted_date  # Changing column name from a_ to posted_date\n",
    "\n",
    "            # Adding job application count if available\n",
    "            applications = more_info[3].strip()  # Changing variable name from s_ to applications\n",
    "            offer_dict[\"applications\"] = applications  # Changing column name from s_ to applications\n",
    "\n",
    "            # Adding job modality if available\n",
    "            modality = soup.find('span', class_='ui-label--accent-3')\n",
    "            if modality:\n",
    "                job_modality = modality.text.strip()\n",
    "                offer_dict[\"job_modality\"] = job_modality\n",
    "\n",
    "            # Adding information about full/part-time status if available\n",
    "            full_part_time = soup.find('span', class_=\"job-details-jobs-unified-top-card__job-insight-view-model-secondary\")\n",
    "            if full_part_time:\n",
    "                f_p_time = full_part_time.text.strip()\n",
    "                offer_dict[\"f_p_time\"] = f_p_time\n",
    "\n",
    "            # Adding job responsibilities if available\n",
    "            responsability = soup.find(\"li\", class_=\"job-details-jobs-unified-top-card__job-insight job-details-jobs-unified-top-card__job-insight--highlight\")\n",
    "            if responsability:\n",
    "                job_responsibility = responsability.text.strip().split(\"\\n\")[-1]\n",
    "                offer_dict[\"job_responsibility\"] = job_responsibility\n",
    "\n",
    "            # Adding required job skills if available\n",
    "            skills = soup.find_all('a', class_='job-details-how-you-match__skills-item-subtitle')\n",
    "            if skills:\n",
    "                job_skills = [skill.text.strip() for skill in skills]\n",
    "                offer_dict[\"job_skills\"] = ', '.join(job_skills)\n",
    "\n",
    "            # Adding job salary if available\n",
    "            salary = soup.find('p', class_='t-16')\n",
    "            if salary:\n",
    "                job_salary = salary.getText().strip()\n",
    "                offer_dict[\"job_salary\"] = job_salary\n",
    "\n",
    "            # Append the offer dictionary to the offers list if it's not a duplicate\n",
    "            if offer_dict not in offers:\n",
    "                offers.append(offer_dict)\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    # Creating a DataFrame from the offers list\n",
    "    df_all_info = pd.DataFrame(offers)\n",
    "    \n",
    "    # Saving DataFrame to a CSV file\n",
    "    df_all_info.to_csv(\"data/df_all_info.csv\", index=False)\n",
    "\n",
    "    return df_all_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94df7e",
   "metadata": {},
   "source": [
    "# Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb247ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_links_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_links_search\n\u001b[0;32m    155\u001b[0m driver \u001b[38;5;241m=\u001b[39m selenium_driver()\n\u001b[1;32m--> 156\u001b[0m df \u001b[38;5;241m=\u001b[39m all_together(driver, num_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    157\u001b[0m df\n",
      "Cell \u001b[1;32mIn[5], line 145\u001b[0m, in \u001b[0;36mall_together\u001b[1;34m(driver, num_pages)\u001b[0m\n\u001b[0;32m    142\u001b[0m df_links_search \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinks_and_query.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m#2. Get html from each link\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m df_links_search \u001b[38;5;241m=\u001b[39m add_html_column (driver)\n\u001b[0;32m    146\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m#3. Get info from each html\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m, in \u001b[0;36madd_html_column\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_html_column\u001b[39m (driver):\n\u001b[1;32m---> 86\u001b[0m     df_links_search[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_links_search[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob_Link\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m link: get_html_from_link(link, driver\u001b[38;5;241m=\u001b[39mdriver))\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_links_search\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_links_search' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"fer\"\"\"\n",
    "\n",
    "def selenium_driver ():\n",
    "    # Get login credentials\n",
    "    email = os.getenv(\"email\")\n",
    "    password = os.getenv(\"password\")\n",
    "    \n",
    "    # WebDriver setup\n",
    "    options = webdriver.ChromeOptions() \n",
    "    options.add_argument('--incognito')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(random.randint(1, 3))  \n",
    "    \n",
    "    email_box = driver.find_element(By.ID, \"username\")\n",
    "    password_box = driver.find_element(By.ID, \"password\")\n",
    "    email_box.send_keys(email)\n",
    "    password_box.send_keys(password)\n",
    "    password_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(random.randint(1, 7))  \n",
    "\n",
    "    return driver\n",
    "\n",
    "def get_links(driver, num_pages=25):\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn job search pages for specific keywords and locations.\n",
    "    \n",
    "    :param num_pages: Number of job search pages to scrape (default 25).\n",
    "    :return: List of BeautifulSoup objects for each job page scraped, and a Data Frame with all the links of job pages\n",
    "    \"\"\"\n",
    "    # Keywords and Locations\n",
    "    keywords = [\n",
    "        \"Data Analyst\",\n",
    "        \"Data Engineer\",\n",
    "        \"Data Scientist\"\n",
    "    ]\n",
    "\n",
    "    locations = [\n",
    "        \"Spain\",\n",
    "        \"England\",\n",
    "        \"Scotland\",\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # List to store BeautifulSoup objects of job pages\n",
    "    soup_list = []\n",
    "\n",
    "    # DataFrame to store job links with additional explanations\n",
    "    #df_links = pd.DataFrame(columns=['Keyword', 'Location', 'Job_Link'])\n",
    "    new_list = []\n",
    "    for keyword in keywords:\n",
    "        for location in locations:\n",
    "            # Build LinkedIn job search URL\n",
    "            base_url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start=\"\n",
    "\n",
    "            new_soup_list = []\n",
    "\n",
    "            for page in range(num_pages):\n",
    "                page_url = base_url + str(page * 25)\n",
    "                driver.get(page_url)\n",
    "                time.sleep(random.uniform(3, 7))\n",
    "\n",
    "                # Extract job links\n",
    "                links = driver.find_elements(By.CSS_SELECTOR, \"a.job-card-container__link.job-card-list__title\")\n",
    "                job_links = [link.get_attribute(\"href\") for link in links]\n",
    "                for job_link in job_links:\n",
    "                    dict_ = {'Keyword': keyword, \n",
    "                                                'Location': location, \n",
    "                                                'Job_Link': job_link}\n",
    "                    if dict_ not in new_list:\n",
    "                        new_list.append(dict_)\n",
    "    df_links_search = pd.DataFrame(new_list)\n",
    "    df_links_search.to_csv(\"links_and_query.csv\", index=False)\n",
    "    return df_links_search\n",
    "\n",
    "              \n",
    "def get_html_from_link (link, driver):\n",
    "   \n",
    "    driver.get(link)\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def add_html_column (driver):\n",
    "    df_links_search[\"html\"] = df_links_search[\"Job_Link\"].apply(lambda link: get_html_from_link(link, driver=driver))\n",
    "    return df_links_search\n",
    "\n",
    "def scrape_job_details(soup):\n",
    "    \"\"\"\n",
    "    Extracts job details from a list of BeautifulSoup objects.\n",
    "\n",
    "    :param soup: List of BeautifulSoup objects for each job page scraped.\n",
    "    :return: DataFrame containing extracted job details.\n",
    "    \n",
    "    \"\"\"\n",
    "    oferta = []  # Initialize an empty list to store job details\n",
    "     # Iterate over each BeautifulSoup object in soup_list\n",
    "    \n",
    "    \n",
    "\n",
    "    # Extract company information\n",
    "    company = soup.find('div', class_='p5')\n",
    "    title = company.find(\"h1\", {\"class\": \"t-24 t-bold job-details-jobs-unified-top-card__job-title\"}).getText()\n",
    "    sub = company.find(\"div\", {\"class\": \"job-details-jobs-unified-top-card__primary-description-container\"})\n",
    "    modality = soup.find('li', class_='job-details-jobs-unified-top-card__job-insight job-details-jobs-unified-top-card__job-insight--highlight').getText()\n",
    "    modality_job = modality.strip().split(\"La modalidad laboral es \")[1].split(\".\")[0]\n",
    "    salary = np.nan\n",
    "   \n",
    "\n",
    "    # Extract job link and company name if available\n",
    "    if sub.find(\"a\"):\n",
    "        link = sub.find(\"a\").get(\"href\")\n",
    "\n",
    "    if sub.find(\"a\"):\n",
    "        empresa = sub.find(\"a\").getText()\n",
    "        \n",
    "\n",
    "    # Extract additional information\n",
    "    more_info = sub.getText().strip().split(\"·\")\n",
    "\n",
    "    c_ = more_info[0].strip()  # Company name\n",
    "    l_ = more_info[1].strip()  # Location\n",
    "\n",
    "    a_ = more_info[2].strip()  # Employment type\n",
    "    s_ = more_info[3].strip()  # Experience level\n",
    "\n",
    "    scraped_on = today = date.today()  # Add scraped date\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    if dict_ not in oferta:\n",
    "        oferta.append(dict_)\n",
    "\n",
    "    return title, empresa, link, c_, l_, a_, s_, scraped_on, modality_job, salary\n",
    "\n",
    "\n",
    "\n",
    "def all_together (driver, num_pages=25):\n",
    "    \n",
    "    #1. Build links and query dataset\n",
    "    #df_links_search = get_links(driver, num_pages=num_pages)\n",
    "    df_links_search = pd.read_csv(\"links_and_query.csv\")\n",
    "    \n",
    "    #2. Get html from each link\n",
    "    df_links_search = add_html_column (driver)\n",
    "    driver.quit()\n",
    "   \n",
    "    #3. Get info from each html\n",
    "    df_links_search[[\"title\", \"company\", \"link\", \"c_\", \"l_\", \"a_\", \"s_\", \"scraped_on\", \"modality_job\", \"salary\"]] = df_links_search[\"html\"].apply(scrape_job_details)\n",
    "    \n",
    "    # 4. Export\n",
    "    df_links_search.to_csv(\"todojunto.csv\", index=False)\n",
    "    return df_links_search\n",
    "\n",
    "driver = selenium_driver()\n",
    "df = all_together(driver, num_pages=10)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb5d9e",
   "metadata": {},
   "source": [
    "# First Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7183c13",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"fer\"\"\"\n",
    "\n",
    "def selenium_driver ():\n",
    "    # Get login credentials\n",
    "    email = os.getenv(\"email\")\n",
    "    password = os.getenv(\"password\")\n",
    "    \n",
    "    # WebDriver setup\n",
    "    options = webdriver.ChromeOptions() \n",
    "    options.add_argument('--incognito')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(random.randint(1, 3))  \n",
    "    \n",
    "    email_box = driver.find_element(By.ID, \"username\")\n",
    "    password_box = driver.find_element(By.ID, \"password\")\n",
    "    email_box.send_keys(email)\n",
    "    password_box.send_keys(password)\n",
    "    password_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(random.randint(1, 7))  \n",
    "\n",
    "    return driver\n",
    "\n",
    "def get_links(driver, num_pages=25):\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn job search pages for specific keywords and locations.\n",
    "    \n",
    "    :param num_pages: Number of job search pages to scrape (default 25).\n",
    "    :return: List of BeautifulSoup objects for each job page scraped, and a Data Frame with all the links of job pages\n",
    "    \"\"\"\n",
    "    # Keywords and Locations\n",
    "    keywords = [\n",
    "        \"Data Analyst\",\n",
    "        \"Data Engineer\",\n",
    "        \"Data Scientist\"\n",
    "    ]\n",
    "\n",
    "    locations = [\n",
    "        \"Spain\",\n",
    "        \"England\",\n",
    "        \"Scotland\",\n",
    "    ]\n",
    "    \n",
    "    \n",
    "\n",
    "    # List to store BeautifulSoup objects of job pages\n",
    "    soup_list = []\n",
    "\n",
    "    # DataFrame to store job links with additional explanations\n",
    "    #df_links = pd.DataFrame(columns=['Keyword', 'Location', 'Job_Link'])\n",
    "    new_list = []\n",
    "    for keyword in keywords:\n",
    "        for location in locations:\n",
    "            # Build LinkedIn job search URL\n",
    "            base_url = f\"https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}&start=\"\n",
    "\n",
    "            new_soup_list = []\n",
    "\n",
    "            for page in range(num_pages):\n",
    "                page_url = base_url + str(page * 25)\n",
    "                driver.get(page_url)\n",
    "                time.sleep(random.uniform(3, 7))\n",
    "\n",
    "                # Extract job links\n",
    "                links = driver.find_elements(By.CSS_SELECTOR, \"a.job-card-container__link.job-card-list__title\")\n",
    "                job_links = [link.get_attribute(\"href\") for link in links]\n",
    "                for job_link in job_links:\n",
    "                    dict_ = {'Keyword': keyword, \n",
    "                                                'Location': location, \n",
    "                                                'Job_Link': job_link}\n",
    "                    if dict_ not in new_list:\n",
    "                        new_list.append(dict_)\n",
    "    df_links_search = pd.DataFrame(new_list)\n",
    "    return df_links_search\n",
    "\n",
    "              \n",
    "def get_html_from_link (driver, link):\n",
    "   \n",
    "    driver.getlink(link)\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def scrape_job_details(soup):\n",
    "    \"\"\"\n",
    "    Extracts job details from a list of BeautifulSoup objects.\n",
    "\n",
    "    :param soup_list: List of BeautifulSoup objects for each job page scraped.\n",
    "    :return: DataFrame containing extracted job details.\n",
    "    \n",
    "    \"\"\"\n",
    "    oferta = []  # Initialize an empty list to store job details\n",
    "\n",
    "     # Iterate over each BeautifulSoup object in soup_list\n",
    "    dict_ = {  # Create a dictionary to store job details with default values as NaN\n",
    "        \"title\": np.nan,\n",
    "        \"company\": np.nan,\n",
    "        \"link\": np.nan,\n",
    "        \"c_\": np.nan,\n",
    "        \"l_\": np.nan,\n",
    "        \"a_\": np.nan,\n",
    "        \"s_\": np.nan\n",
    "    }\n",
    "\n",
    "    # Extract company information\n",
    "    company = soup.find('div', class_='p5')\n",
    "    title = company.find(\"h1\", {\"class\": \"t-24 t-bold job-details-jobs-unified-top-card__job-title\"}).getText()\n",
    "    sub = company.find(\"div\", {\"class\": \"job-details-jobs-unified-top-card__primary-description-container\"})\n",
    "\n",
    "    # Extract job link and company name if available\n",
    "    if sub.find(\"a\"):\n",
    "        link = sub.find(\"a\").get(\"href\")\n",
    "        dict_[\"link\"] = link\n",
    "\n",
    "    if sub.find(\"a\"):\n",
    "        empresa = sub.find(\"a\").getText()\n",
    "        dict_[\"company\"] = empresa\n",
    "\n",
    "    # Extract additional information (location, employment type, experience level, and job function)\n",
    "    more_info = sub.getText().strip().split(\"·\")\n",
    "\n",
    "    c_ = more_info[0].strip()  # Company name\n",
    "    l_ = more_info[1].strip()  # Location\n",
    "\n",
    "    a_ = more_info[2].strip()  # Employment type\n",
    "    s_ = more_info[3].strip()  # Experience level\n",
    "\n",
    "    # Update the dictionary with extracted job details\n",
    "    dict_[\"title\"] = title\n",
    "    dict_[\"c_\"] = c_\n",
    "    dict_[\"l_\"] = l_\n",
    "    dict_[\"a_\"] = a_\n",
    "    dict_[\"s_\"] = s_\n",
    "\n",
    "    dict_[\"scraped_on\"] = today = date.today()  # Add scraped date\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    if dict_ not in oferta:\n",
    "        oferta.append(dict_)\n",
    "\n",
    "    # Create a DataFrame from the list of job details\n",
    "    df = pd.DataFrame(oferta)\n",
    "    return df\n",
    "\n",
    "driver = selenium_driver()\n",
    "df_links_search = get_links(driver, num_pages=25)\n",
    "df_links_search[\"html\"] = df[\"job_link\"].apply(get_html_from_link(driver))\n",
    "df_links_search[[\"title\", \"company\", \"link\", \"c_\", \"l_\", \"a_\", \"s_\"]] = df[\"job_link\"].apply(scrape_job_details, axis=1)\n",
    "driver.quit()\n",
    "df.to_csv(\"todojunto.csv\", index=False)\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c0aff",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
